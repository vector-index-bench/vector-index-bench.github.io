[
  {
    "objectID": "datasets.html",
    "href": "datasets.html",
    "title": "VIBE: Vector Index Benchmark for Embeddings",
    "section": "",
    "text": "In the VIBE benchmark we consider datasets whose queries are both  in-distribution  and  out-of-distribution.\nThe following table reports information about the size and dimensionality of each dataset, along with links that allow to download them.\ndb = DuckDBClient.of({\n    vizdata: FileAttachment(\"results/data-pca-mahalanobis.parquet\"),\n    stats: FileAttachment(\"results/stats.parquet\"),\n  basics: {file: FileAttachment(\"dataset_basics.csv\"), header: true},\n  info: FileAttachment(\"results/data-info.parquet\")\n});\nid_datasets = db.sql`select * from basics natural join info where type in ('in-distribution', 'in-distribution-additional') order by dataset`;\nood_datasets = db.sql`select * from basics natural join info where type = 'out-of-distribution' order by dataset`;\ndatasets = db.sql`select * from basics natural join info where type in ('in-distribution', 'in-distribution-additional', 'out-of-distribution') order by type, dataset`;\n\ncolors = Array.from(d3[\"schemeTableau10\"]).toReversed();\ntype_colors = new Map([\n  [\"in-distribution\", \"#1f77b4\"],\n  [\"in-distribution-additional\", \"#1f77b4\"],\n  [\"out-of-distribution\", \"#ff7f0e\"]\n])\n\nviewof selected_dataset = Inputs.table(datasets,{\n  format: {\n    dataset: dataset =&gt; html`&lt;a href=\"https://huggingface.co/datasets/ejaasaari/annbench/blob/main/${dataset}.hdf5\"&gt;${dataset}&lt;/a&gt;`,\n    type: t =&gt; html`&lt;div style=\"\n      width: 3ex;\n      height: 3ex;\n      background: ${type_colors.get(t)}\n    \"&gt;&lt;/div&gt;`\n  },\n  width: {\n    dataset: 280\n  },\n  multiple: false,\n  value: datasets[0]\n})\nHere below we report the first two PCA components of  data  and  queries  for each dataset. Selecting a dataset in the table above allows to update the visualization.\nAlong with the PCA we display the distribution of Mahalanobis distances between the  data points  and the data and the  query points  and the data.\nselected_data = db.sql`select * from vizdata where dataset = ${selected_dataset.dataset}`;\nkde = require('fast-kde');\ndistances_data = db.sql`select mahalanobis_distance_to_data from vizdata where dataset = ${selected_dataset.dataset} and part = 'train'`;\ndensity_data = kde.density1d(distances_data.map(d =&gt; d.mahalanobis_distance_to_data), { bandwidth: 3, bins: 512, pad: 4 });\ndistances_query = db.sql`select mahalanobis_distance_to_data from vizdata where dataset = ${selected_dataset.dataset} and part = 'test'`;\ndensity_query = kde.density1d(distances_query.map(d =&gt; d.mahalanobis_distance_to_data), { bandwidth: 3, bins: 512, pad: 4 });\n\nbw = 1;\ndensity_transform = function(data, facets) {\n  // Separately compute densities for each facet\n  const densities = facets.map((facet) =&gt; {\n    // `facet` is a `UInt32Array`, so it needs to be copied before we can use it like this\n    const facetData = [...facet].map((index) =&gt; data[index]);\n    const iter = (new Set(facetData.map(d =&gt; d.dataset))).values();\n    const dataset = iter.next().value;\n    console.log(dataset);\n\n    // `density1d` also needs to be copied to produce (x, y) pairs for our plot\n    const densityData = [\n      ...kde\n        .density1d(facetData, {\n          x: \"rc100\",\n          pad,\n          bins,\n          //extent: [0, 3]\n        })\n        .points(\"point\", \"density\")\n    ];\n\n    densityData.forEach(d =&gt; d[\"dataset\"] = dataset);\n    return densityData;\n  });\n\n  // Simultaneously flatten the array and compute the new facets\n\n  let newData = [];\n  let newFacets = [];\n  let index = 0;\n\n  for (let density of densities) {\n    let facet = [];\n    for (let facetIndex = 0; facetIndex &lt; density.length; facetIndex++) {\n      newData.push(density[facetIndex]);\n      facet.push(index);\n      index++;\n    }\n    newFacets.push(facet);\n  }\n\n  return {\n    data: newData,\n    facets: newFacets\n  };\n}\nTo characterize the difficulty of queries, and hence of the workloads associated with each dataset, we consider the relative contrast, defined as [ RC_k = ] where \\(d_{avg}\\) is the average distance of the query to the other points, and \\(d_k\\) is the distance of the query from its \\(k\\)-th nearest neighbor.\nThe plot below reports the distribution of relative contrasts for \\(k=100\\) for the datasets1 in the benchmark, with datasets arranged in increasing order of difficulty, top to bottom."
  },
  {
    "objectID": "datasets.html#footnotes",
    "href": "datasets.html#footnotes",
    "title": "VIBE: Vector Index Benchmark for Embeddings",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDatasets with inner product similarity are omitted from the plot, as the inner product is not a metric, and the relative constrast is not well-defined for non-metric distances.↩︎"
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "VIBE: Vector Index Benchmark for Embeddings",
    "section": "",
    "text": "db = DuckDBClient.of({\n  summary: FileAttachment(\"results/summary.parquet\"),\n  stats: FileAttachment(\"results/stats.parquet\"),\n  basics: {file: FileAttachment(\"dataset_basics.csv\"), header: true},\n  algorithm_basics: {file: FileAttachment(\"algorithms_basics.csv\"), header: true}\n});\nk_values = db.sql`select distinct k from summary;`\nviewof recall_threshold = Inputs.range([0,1], {step: 0.01, value: 0.9, label: \"minimum recall\"});\nviewof k_value = Inputs.select(k_values.map(d =&gt; d.k), {value: 10, label: \"value of k\"});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfastdata = db.sql`\n  with\n    normalized_names as (\n      select k, regexp_replace(dataset, '-(a2|e2)-', '-') as dataset, algorithm, params, avg_time, qps, recall from summary\n    ),\n    selected_datasets as (select dataset, type from basics where type in ('in-distribution', 'out-of-distribution') ),\n    filtered_summary as (\n      select * from normalized_names natural join selected_datasets natural left join algorithm_basics\n      where not is_gpu \n    ),\n    all_datasets as ( select distinct dataset, type from filtered_summary ),\n    all_algorithms as ( select distinct algorithm from filtered_summary ),\n    expected_combinations as (\n      select *\n      from all_datasets cross join all_algorithms\n    ),\n    ranked as (\n        select *, row_number() over (partition by algorithm, dataset order by qps desc) as rank\n        from filtered_summary\n        where recall &gt;= ${recall_threshold} and k = ${k_value}\n    ),\n    scaled as (\n      select\n        *,\n        ifnull(qps, 0) / max(qps) over (partition by dataset, type) as scaled_qps\n      from ranked\n        natural right join expected_combinations\n    ),\n    best_performing as (\n      select algorithm, dataset, max(scaled_qps) as best_performance\n      from scaled\n      group by all\n    ),\n    algorithms as (\n      select algorithm, avg(best_performance) as mean_perf\n      from best_performing\n      group by all\n    ),\n    algorithms_ranks as (\n      select\n        algorithm, mean_perf,\n        row_number() over (order by mean_perf desc) as algorithm_rank\n      from algorithms\n    ),\n    datasets as (\n      select dataset, type, avg(rc100) as difficulty\n      from stats natural join selected_datasets\n      group by all\n    ),\n    dataset_ranks as (\n      select\n        dataset, difficulty,\n        row_number() over (order by type, contains(dataset, '-ip'), difficulty desc) as dataset_rank\n      from datasets \n    )\n  select\n    algorithm,\n    regexp_replace(dataset, '-[0-9]+-(cosine|normalized|euclidean|ip)', '') as dataset,\n    type as dataset_type,\n    dataset_rank,\n    k,\n    scaled_qps,\n    qps,\n    params,\n    (algorithm_rank / 6) as fx,\n    (algorithm_rank % 6) as fy\n  from scaled natural join algorithms_ranks natural join dataset_ranks\n  where rank = 1 or rank is null\n  order by dataset_rank, algorithm, qps;\n`\n\nfacet_keys = Array.from(d3.union(fastdata.map((d) =&gt; d.algorithm)));\n\ndataset_ranks = fastdata.reduce((map, d) =&gt; {\n  map[d.dataset] = d.dataset_rank;\n  return map;\n}, {});\ndataset_types = fastdata.reduce((map, d) =&gt; {\n  map[d.dataset] = d.dataset_type;\n  return map;\n}, {});\n\nlongitude_domain = Object.keys(dataset_ranks).sort((a, b) =&gt; dataset_ranks[a] - dataset_ranks[b]);\n\n// Scales\nlongitude = d3.scalePoint(\n  longitude_domain,\n  [180, -180]\n).padding(0.5).align(1);\nfmt_qps = d3.format(\".2s\");\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  width: width,\n  height: 3*width/4,\n  marginTop: 20,\n  marginBottom: 20,\n  marginLeft: 20,\n  marginRight: 20,\n  projection: {\n    type: \"azimuthal-equidistant\",\n    rotate: [0, -90],\n    // Note: 1.22° corresponds to max. percentage (1.0), plus some room for the labels\n    domain: d3.geoCircle().center([0, 90]).radius(1.22)()\n  },\n  facet: {\n    data: fastdata,\n    y: (d) =&gt; d.fx, //Math.floor(d.algorithm_rank / 4),\n    x: (d) =&gt; d.fy, // d.algorithm_rank % 4,\n    axis: null\n  },\n  fx: {padding: .1},\n  fy: {padding: .1},\n  marks: [\n    // Facet name\n    Plot.text(fastdata,\n      Plot.selectFirst({\n          text: \"algorithm\",\n          frameAnchor: \"top-left\",\n          fontWeight: \"700\",\n          fontSize: 24\n      })\n    ),\n\n    // grey discs\n    Plot.geo([1.0, 0.8, 0.6, 0.4, 0.2], {\n      geometry: (r) =&gt; d3.geoCircle().center([0, 90]).radius(r)(),\n      stroke: \"black\",\n      fill: \"black\",\n      strokeOpacity: 0.2,\n      fillOpacity: 0.03,\n      strokeWidth: 0.5\n    }),\n\n    // colored axes\n    Plot.link(longitude.domain(), {\n      x1: longitude,\n      y1: 90 - 1,\n      x2: 0,\n      y2: 90,\n      stroke: d =&gt; (dataset_types[d] == \"in-distribution\")? \"#1f77b4\" : \"#ff7f0e\",\n      strokeOpacity: 0.5,\n      strokeWidth: 2.5\n    }),\n\n    // tick labels\n    Plot.text([0.4, 0.6, 0.8], {\n      fx: 0, fy: 0,\n      x: 180,\n      y: (d) =&gt; 90 - d,\n      dx: 2,\n      textAnchor: \"start\",\n      text: (d) =&gt; ( d == 0.8 ? `${100 * d}%` : `${100 * d}%`),\n      fill: \"currentColor\",\n      stroke: \"white\",\n      fontSize: 18\n    }),\n\n    // axes labels\n    Plot.text(longitude.domain(), {\n      fx: 0, fy: 0,\n      x: longitude,\n      y: 90 - 1.07,\n      text: Plot.identity,\n      lineWidth: 10,\n      fontSize: 18\n    }),\n\n    // axes labels, initials\n    Plot.text(longitude.domain(), {\n      fx: 0, fy: 0, facet: \"exclude\",\n      x: longitude,\n      y: 90 - 1.09,\n      text: d =&gt; d.slice(0,2),\n      lineWidth: 5,\n      fontSize: 18\n    }),\n\n    // areas\n    Plot.area(fastdata, {\n      x1: ({ dataset }) =&gt; longitude(dataset),\n      y1: ({ scaled_qps }) =&gt; 90 - scaled_qps,\n      x2: 0,\n      y2: 90,\n      fill: \"gray\",\n      fillOpacity: 0.25,\n      stroke: \"gray\",\n      curve: \"cardinal-closed\"\n    }),\n\n    // data values\n    Plot.dot(fastdata, {\n      x: (d) =&gt; longitude(d.dataset),\n      y: (d) =&gt; 90 - d.scaled_qps,\n      fill: (d) =&gt; (d.dataset_type == \"in-distribution\")? \"#1f77b4\" : \"#ff7f0e\",\n      stroke: \"white\"\n    }),\n\n    // interactive labels\n    Plot.text(\n      fastdata,\n      Plot.pointer({\n        x: ({ dataset }) =&gt; longitude(dataset),\n        y: ({ scaled_qps }) =&gt; 90 - scaled_qps,\n        text: (d) =&gt; `${fmt_qps(d.qps)} qps\\n(${Math.round(100 * d.scaled_qps)}%)\\n${d.params}`,\n        textAnchor: \"start\",\n        dx: 4,\n        fill: \"currentColor\",\n        stroke: \"white\",\n        maxRadius: 10,\n        fontSize: 18\n      })\n    )\n    \n  ]\n})"
  },
  {
    "objectID": "robustness.html",
    "href": "robustness.html",
    "title": "VIBE: Vector Index Benchmark for Embeddings",
    "section": "",
    "text": "This page allows to select, for each dataset, a recall threshold. Then, for each algorithm, the fastest configuration achieving an average recall larger than the threshold is automatically selected.\nWe then consider two groups of queries:\n\nthe 100 queries with smallest relative contrast, deemed  difficult\nthe 100 queries with largest relative contrast, deemed  easy \n\nThe plot below report the average recall attained by each algorithm on both groups, under the aforementioned configuration.\n\ndb = DuckDBClient.of({\n  stats: FileAttachment(\"results/stats.parquet\"),\n  summary: FileAttachment(\"results/summary.parquet\"),\n  basics: {file: FileAttachment(\"dataset_basics.csv\"), header: true},\n  gpu_algorithms: {file: FileAttachment(\"gpu_algorithms.csv\"), header: true}\n})\n\ndatasets = db.sql`select distinct dataset from summary order by all`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof selected_dataset = Inputs.select(datasets.map(d =&gt; d.dataset))\nk_value = 100\nviewof recall_threshold = Inputs.range([0,1], {step: 0.01, value: 0.9, label: \"minimum recall\"});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfocus_url = `${window.location.origin}${window.location.pathname.slice(0, -\"robustness.html\".length)}results/${selected_dataset}__detail.parquet`;\n\n\n\n\n\n\n\nstats = db.sql`\nWITH\n  query_ranks AS (\n    select \n      query_index, \n      rc100, \n      rank() over (order by rc100) as query_rank,\n    from stats where dataset = ${selected_dataset}),\n  query_type AS (\n    select\n      query_index,\n      rc100,\n      case when query_rank &lt; 100 then 'difficult' when query_rank &gt;= 900 then 'easy' end as query_type\n    from query_ranks\n  )\nSELECT * from query_type where query_type in ('difficult', 'easy')\n`\n\n\n\n\n\n\n\nselected = db.query(`\n  with\n    normalized_names as (\n      select k, regexp_replace(dataset, '-(a2|e2)-', '-') as dataset, algorithm, params, avg_time, qps, recall from summary\n    ),\n    filtered_summary as (\n      select * from normalized_names natural left join gpu_algorithms\n    ),\n    ranked as (\n        select *, row_number() over (partition by algorithm, dataset order by qps desc) as rank\n        from filtered_summary\n        where recall &gt;= ?3 and k = ?2\n          and dataset = ?1 \n    ),\n    params as (\n      select algorithm, params from ranked where rank = 1\n    ),\n    query_ranks AS (\n      select \n        query_index, \n        rc100, \n        rank() over (order by rc100) as query_rank,\n      from stats where dataset = ?1),\n    query_type AS (\n      select\n        query_index,\n        rc100,\n        case when query_rank &lt; 100 then 'difficult' when query_rank &gt;= 900 then 'easy' end as query_type\n      from query_ranks\n    ),\n    query_perf AS (\n      select * from '${focus_url}' natural join params where dataset = ?1 and k = ?2\n    )\n    select algorithm, query_type, avg(recall) as recall\n    from query_perf natural join query_type\n    where query_type in ('difficult', 'easy')\n    group by all\n`, [selected_dataset, k_value, recall_threshold])\n\n\n\n\n\n\n\nPlot.plot({\n  width: width,\n  style: {\n    fontSize: \"13pt\"\n  },\n  axis: null,\n  marginRight: 200,\n  marginLeft: 60,\n  marks: [\n    Plot.ruleX([recall_threshold], {stroke: \"lightgray\"}),\n    Plot.link(selected, Plot.groupY(\n      {\n        x1: (D) =&gt; d3.min(D),\n        x2: (D) =&gt; d3.max(D)\n      },\n      {\n        y: \"algorithm\",\n        x1: \"recall\",\n        x2: \"recall\"\n      }\n    )),\n    Plot.dot(selected, {\n      x: \"recall\", \n      y: \"algorithm\", \n      fill: \"query_type\",\n      r: 5,\n      sort: {y: \"-x\"}\n    }),\n    Plot.text(selected, Plot.groupY(\n      {\n        x: D =&gt; d3.max(D),\n        text: D =&gt; D[0],\n      },\n      {\n        y: \"algorithm\",\n        x: \"recall\",\n        text: \"algorithm\",\n        dx: 70,\n        textAnchor: \"start\",\n        lineAnchor: \"middle\"\n      }\n    )),\n    Plot.text(selected, Plot.groupY(\n      {\n        x: D =&gt; d3.min(D),\n        text: D =&gt; d3.format(\".3f\")(d3.min(D)),\n      },\n      {\n        y: \"algorithm\",\n        x: \"recall\",\n        text: \"recall\",\n        dx: -10,\n        textAnchor: \"end\"\n      }\n    )),\n    Plot.text(selected, Plot.groupY(\n      {\n        x: D =&gt; d3.max(D),\n        text: D =&gt; d3.format(\".3f\")(d3.max(D)),\n      },\n      {\n        y: \"algorithm\",\n        x: \"recall\",\n        text: \"recall\",\n        dx: 10,\n        textAnchor: \"start\"\n      }\n    )),\n  ]\n})"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "VIBE: Vector Index Benchmark for Embeddings",
    "section": "",
    "text": "This website hosts the results of VIBE, the Vector Index Benchmark for Embeddings. You can use its interactive plots to explore the results of the benchmark. In particular, head over to\n\nthe overview page to compare all the algorithms across all datasets at a glance\nthe datasets page for detailed information about all the datasets we consider\nthe tradeoffs page to explore the tradeoff between recall and query throughput of different index data structures\nthe algorithm focus page to see the performance of all the configurations we tested for each algorithm.\nthe robustness page allows to investigate the robustness of algorithms with respect to the query difficulty."
  }
]